import torch
from transformers import BartTokenizer, BartForConditionalGeneration
from logs.logger import logging, logger_mem
from datetime import date
import uuid
from mem_db.vecto import get_or_create_collection


# Détection automatique du device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Paramètres globaux
MAX_MEMORY_SIZE = 2000  # Limite du nombre de messages
MAX_TOKENS_IN_MEMORY = 1000  # Limite pour compresser la mémoire
BATCH_SIZE = 5  # Taille du batch pour la compression

class BartSingleton:
    _instance = None

    def __new__(cls):
        if cls._instance is None:
            logging.info("Instanciation du modèle BART...")
            cls._instance = super(BartSingleton, cls).__new__(cls)
            cls._instance.tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')
            cls._instance.model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn').to(device)
        return cls._instance
    
    @classmethod
    def reset(cls):
        """
        Resets the BART singleton, allowing it to be reinitialized.
        This should only be used for testing purposes.
        """
        logging.warning("Reset du singleton BART.")
        cls._instance = None

class ChatbotMemory:
    def __init__(self, conv:list=None):
        self.conversation_history = conv or []
        bart = BartSingleton()
        self.tokenizer = bart.tokenizer
        self.model = bart.model

        self.persistent_storage = get_or_create_collection("conv_memory")

    def update_memory(self, user_input:str, bot_response:str)->None:
        """
        Updates the conversation history in the Chatbot's memory with the user input and bot response.
        Args:
            user_input (str): The input provided by the user.
            bot_response (str): The response generated by the Chatbot.
        Returns:
            None        """
        self.conversation_history.append({'user': user_input, 'bot': bot_response})
        # date.today()

        self.persistent_storage.add(
        documents=[f"user: {user_input} bot: {bot_response}"],
        ids=[str(uuid.uuid4())],
        metadatas=[{"type": "chat_entry"}]
        )

        if self.memory_counter() > MAX_TOKENS_IN_MEMORY:
            self.conversation_history = self.compressed_memory()
            logger_mem.info("Mémoire compressée.")

        if len(self.conversation_history) > MAX_MEMORY_SIZE:
            self.conversation_history.pop(0)
            logging.info("Mémoire tronquée.")

    def get_memory(self):
        """
        Returns the conversation history stored in the Chatbot's memory.

        Returns:
            The conversation history.
        """
        return self.conversation_history

    def _get_compressed_memory(self, text:str):
        """
        Résume un bloc de texte.
        """
        inputs = self.tokenizer(
            f"summarize: {text}",
            return_tensors="pt",
            max_length=1024,
            truncation=True,
        ).to(device)

        summary_ids = self.model.generate(
            inputs.input_ids,
            max_length=150,
            min_length=40,
            length_penalty=2.0,
            num_beams=4,
            early_stopping=True
        )
        return self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)

    def compressed_memory(self):
        """
        Résume l'historique de la conversation par batch de BATCH_SIZE.
        """
        combined_history = [f"User: {entry['user']} Bot: {entry['bot']}" for entry in self.conversation_history]
        return [
            {'summary': self._get_compressed_memory(' '.join(combined_history[i:i+BATCH_SIZE]))}
            for i in range(0, len(combined_history), BATCH_SIZE)
        ]

    def memory_counter(self):
        """
        Compte le nombre total de tokens dans l'historique.
        """
        all_text = ' '.join(f"{entry['user']} {entry['bot']}" for entry in self.conversation_history)
        return len(self.tokenizer.tokenize(all_text))

if __name__ == "__main__":
    chat_memory = ChatbotMemory()
    
    chat_memory.update_memory("Bonjour, comment allez-vous?", "Je vais bien, merci ! Et vous ?")
    chat_memory.update_memory("Écris un poème de 300 mots", "Sous l'éclat argenté d'une lune éternelle... (poème)")

    historique = chat_memory.get_memory()
    for entry in historique:
        print(entry)
