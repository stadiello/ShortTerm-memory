import torch
from transformers import BartTokenizer, BartForConditionalGeneration
import logging

# Configuration du logging
logging.basicConfig(level=logging.INFO)

# Détection automatique du device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Paramètres globaux
MAX_MEMORY_SIZE = 2000  # Limite du nombre de messages
MAX_TOKENS_PER_MESSAGE = 1000  # Limite pour compresser la mémoire
BATCH_SIZE = 5  # Taille du batch pour la compression

class ChatbotMemory:
    def __init__(self, conv:list=None):
        self.conversation_history = conv or []
        self.tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')
        self.model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn').to(device)

    def update_memory(self, user_input:str, bot_response:str)->None:
        """
        Updates the conversation history in the Chatbot's memory with the user input and bot response.
        Args:
            user_input (str): The input provided by the user.
            bot_response (str): The response generated by the Chatbot.
        Returns:
            None        """
        self.conversation_history.append({'user': user_input, 'bot': bot_response})

        if self.memory_counter() > MAX_TOKENS_PER_MESSAGE:
            self.conversation_history = self.compressed_memory()
            logging.info("Mémoire compressée.")

        if len(self.conversation_history) > MAX_MEMORY_SIZE:
            self.conversation_history.pop(0)
            logging.info("Mémoire tronquée.")

    def get_memory(self):
        """
        Returns the conversation history stored in the Chatbot's memory.

        Returns:
            The conversation history.
        """
        return self.conversation_history

    def _get_compressed_memory(self, text:str):
        """
        Résume un bloc de texte.
        """
        inputs = self.tokenizer(
            f"summarize: {text}",
            return_tensors="pt",
            max_length=1024,
            truncation=True,
        ).to(device)

        summary_ids = self.model.generate(
            inputs.input_ids,
            max_length=150,
            min_length=40,
            length_penalty=2.0,
            num_beams=4,
            early_stopping=True
        )
        return self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)

    def compressed_memory(self):
        """
        Résume l'historique de la conversation par batch de BATCH_SIZE.
        """
        combined_history = [f"User: {entry['user']} Bot: {entry['bot']}" for entry in self.conversation_history]
        return [
            {'summary': self._get_compressed_memory(' '.join(combined_history[i:i+BATCH_SIZE]))}
            for i in range(0, len(combined_history), BATCH_SIZE)
        ]

    def memory_counter(self):
        """
        Compte le nombre total de tokens dans l'historique.
        """
        all_text = ' '.join(f"{entry['user']} {entry['bot']}" for entry in self.conversation_history)
        return len(self.tokenizer.tokenize(all_text))

if __name__ == "__main__":
    chat_memory = ChatbotMemory()
    
    chat_memory.update_memory("Bonjour, comment allez-vous?", "Je vais bien, merci ! Et vous ?")
    chat_memory.update_memory("Écris un poème de 300 mots", "Sous l'éclat argenté d'une lune éternelle... (poème)")

    historique = chat_memory.get_memory()
    for entry in historique:
        print(entry)
